<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Ankit Badatala - Portfolio</title>
  <link rel="stylesheet" href="/styles.css" />
  
  <!-- Preload critical assets for faster rendering -->
  <link rel="preload" href="/objects/marble-floor.webp" as="image" />
  <link rel="preload" href="/objects/videos/marble-typing-astra.mp4" as="video" type="video/mp4" />
</head>
<body>

  <div class="frame">
    <!-- Background typing video - always playing -->
    <video class="vid vid-bg" autoplay muted loop playsinline preload="auto">
      <source src="/objects/videos/marble-typing-astra.mp4" type="video/mp4" />
    </video>
    
    <!-- Overlay videos - shown on top when needed -->
    <video class="vid vid-overlay" id="call-answer-vid" muted playsinline>
      <source src="/objects/videos/call-answer.mp4" type="video/mp4" />
    </video>
    
    <video class="vid vid-overlay" id="call-end-vid" muted playsinline>
      <source src="/objects/videos/call-end.mp4" type="video/mp4" />
    </video>
  </div>

  <!-- Floating Action Buttons -->
  <div class="fab-container">
    <button class="fab download-fab" id="download-fab" aria-label="Download Resume">
      <svg width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M19 9h-4V3H9v6H5l7 7 7-7zM5 18v2h14v-2H5z"/>
      </svg>
      <span class="fab-tooltip">Download Resume</span>
    </button>
    <button class="fab chat-fab" id="chat-fab" aria-label="Open chat">
      <svg width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/>
      </svg>
      <span class="fab-tooltip">Open Chat</span>
    </button>
    <button class="fab call-fab" id="call-fab" aria-label="Start Call">
      <svg class="call-icon" width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20.01 15.38c-1.23 0-2.42-.2-3.53-.56-.35-.12-.74-.03-1.01.24l-1.57 1.97c-2.83-1.35-5.48-3.9-6.89-6.83l1.95-1.66c.27-.28.35-.67.24-1.02-.37-1.11-.56-2.3-.56-3.53 0-.54-.45-.99-.99-.99H4.19C3.65 3 3 3.24 3 3.99 3 13.28 10.73 21 20.01 21c.71 0 .99-.63.99-1.18v-3.45c0-.54-.45-.99-.99-.99z"/>
      </svg>
      <svg class="end-call-icon" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" style="display: none;">
        <path d="M18 6L6 18M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"/>
      </svg>
      <span class="fab-tooltip">Start Call</span>
    </button>
  </div>

  <!-- Chat Window - degree-guru style -->
  <div class="chat-panel" id="chat-panel">
    <!-- Close button -->
    <button class="chat-close-btn" id="chat-close-btn" aria-label="Close chat">
      <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M18 6L6 18M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"/>
      </svg>
    </button>
    <div class="chat-messages" id="chat-messages">
      <!-- Welcome message -->
      <div class="chat-message ai-message">
        <div class="message-avatar ai-avatar">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
          </svg>
        </div>
        <div class="message-content">
          <strong>Welcome to Ankit's Portfolio</strong><br><br>
          Ankit built me to help answer any questions you might have about his career, experience, or projects.
          <div class="example-questions">
            <button class="example-question-btn" data-question="What should I ask you about if I only have 5 minutes?">What should I ask you about if I only have 5 minutes?</button>
            <button class="example-question-btn" data-question="What are Ankit's technical skills?">What are Ankit's technical skills?</button>
            <button class="example-question-btn" data-question="Tell me about his leadership style">Tell me about his leadership style.</button>
            <button class="example-question-btn" data-question="Is Ankit a strong fit for a Voice AI Engineer role?">Is Ankit a strong fit for a Voice AI Engineer role?</button>
            <button class="example-question-btn" data-question="How did he build the image to dashboard computer vision pipeline">How did he build the image to dashboard computer vision pipeline?</button>
          </div>
        </div>
      </div>
    </div>
    
    <div class="chat-input-area">
      <form class="chat-form" id="chat-form">
        <input 
          type="text" 
          class="chat-input" 
          id="chat-input" 
          placeholder="Your question..." 
          autocomplete="off"
          required
        />
        <button type="submit" class="chat-send-btn" id="chat-send-btn">
          <svg class="send-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M22 2L11 13" stroke-linecap="round" stroke-linejoin="round"/>
            <path d="M22 2L15 22L11 13L2 9L22 2Z" stroke-linecap="round" stroke-linejoin="round"/>
          </svg>
          <svg class="stop-icon" width="18" height="18" viewBox="0 0 24 24" fill="currentColor" style="display: none;">
            <rect x="4" y="4" width="16" height="16" rx="2"/>
          </svg>
        </button>
      </form>
    </div>
  </div>


  <!-- Marked.js for markdown rendering -->
  <script src="https://cdn.jsdelivr.net/npm/marked@12.0.0/marked.min.js"></script>
  
  <script>
    const frame = document.querySelector('.frame');
    const bgVideo = document.querySelector('.vid-bg');
    const callAnswerVideo = document.getElementById('call-answer-vid');
    const callEndVideo = document.getElementById('call-end-vid');
    
    // Chat elements
    const chatPanel = document.getElementById('chat-panel');
    const chatForm = document.getElementById('chat-form');
    const chatInput = document.getElementById('chat-input');
    const chatSendBtn = document.getElementById('chat-send-btn');
    const sendIcon = chatSendBtn.querySelector('.send-icon');
    const stopIcon = chatSendBtn.querySelector('.stop-icon');
    const chatMessages = document.getElementById('chat-messages');
    const chatFab = document.getElementById('chat-fab');
    const chatCloseBtn = document.getElementById('chat-close-btn');
    const downloadFab = document.getElementById('download-fab');
    const callFab = document.getElementById('call-fab');
    
    // Call state
    let isInCall = false;
    
    // Chat streaming state
    let isStreaming = false;
    let abortController = null;
    
    // Toggle button between send and stop
    function setStreamingMode(streaming) {
      isStreaming = streaming;
      sendIcon.style.display = streaming ? 'none' : 'block';
      stopIcon.style.display = streaming ? 'block' : 'none';
    }
    
    // Session management for chat history
    let sessionId = null;
    
    // Generate a new session ID
    function generateSessionId() {
      return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
        const r = Math.random() * 16 | 0;
        const v = c === 'x' ? r : (r & 0x3 | 0x8);
        return v.toString(16);
      });
    }
    
    // Initialize session on page load
    sessionId = generateSessionId();
    console.log('Chat session initialized:', sessionId);


    // Hide all overlay videos
    function hideAllOverlays() {
      callAnswerVideo.classList.remove('active');
      callEndVideo.classList.remove('active');
      callAnswerVideo.pause();
      callEndVideo.pause();
    }

    function downloadResume() {
      const link = document.createElement('a');
      link.href = 'objects/Ankits_Badatala_Resume.pdf';
      link.download = 'Ankits_Badatala_Resume.pdf';
      document.body.appendChild(link);
      link.click();
      document.body.removeChild(link);
    }

    downloadFab.addEventListener('click', downloadResume);

    // Toggle chat panel
    function openChat() {
      frame.classList.add('chat-open');
      chatPanel.classList.add('open');
      chatInput.focus();
    }

    function closeChat() {
      frame.classList.remove('chat-open');
      chatPanel.classList.remove('open');
    }

    // Floating chat button
    chatFab.addEventListener('click', () => {
      openChat();
    });

    // Close button in chat panel
    chatCloseBtn.addEventListener('click', () => {
      closeChat();
    });

    // SVG icons for avatars
    const aiAvatarSVG = `<svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
      <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
    </svg>`;
    
    const userAvatarSVG = `<svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
      <path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"/>
    </svg>`;

    // Add message to chat
    function addMessage(content, isUser = false) {
      const messageDiv = document.createElement('div');
      messageDiv.className = `chat-message ${isUser ? 'user-message' : 'ai-message'}`;
      
      messageDiv.innerHTML = `
        <div class="message-avatar ${isUser ? 'user-avatar' : 'ai-avatar'}">${isUser ? userAvatarSVG : aiAvatarSVG}</div>
        <div class="message-content">${content}</div>
      `;
      
      chatMessages.appendChild(messageDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
      
      return messageDiv;
    }

    // Simple markdown to HTML converter (handles basic formatting)
    function markdownToHtml(text) {
      // Convert markdown to HTML
      if (typeof marked !== 'undefined') {
        return marked.parse(text);
      }
      // Fallback: basic formatting if marked.js not loaded
      return text
        .replace(/\n/g, '<br>')
        .replace(/^- (.+)$/gm, '<li>$1</li>')
        .replace(/(<li>.*<\/li>)/s, '<ul>$1</ul>')
        .replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>')
        .replace(/\*(.+?)\*/g, '<em>$1</em>');
    }

    // Create an empty AI message for streaming
    function createStreamingMessage() {
      const messageDiv = document.createElement('div');
      messageDiv.className = 'chat-message ai-message';
      messageDiv.innerHTML = `
        <div class="message-avatar ai-avatar">${aiAvatarSVG}</div>
        <div class="message-content" id="streaming-content"></div>
      `;
      chatMessages.appendChild(messageDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
      return messageDiv.querySelector('#streaming-content');
    }

    // Show typing indicator - degree-guru style bouncing dots
    function showTypingIndicator() {
      const typingDiv = document.createElement('div');
      typingDiv.className = 'message-loading';
      typingDiv.id = 'typing-indicator';
      
      typingDiv.innerHTML = `
        <div class="message-avatar ai-avatar">${aiAvatarSVG}</div>
        <div class="loading-dots">
          <span></span>
          <span></span>
          <span></span>
        </div>
      `;
      
      chatMessages.appendChild(typingDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
    }

    function hideTypingIndicator() {
      const typingDiv = document.getElementById('typing-indicator');
      if (typingDiv) {
        typingDiv.remove();
      }
    }

    // Handle stop button click
    function stopStreaming() {
      if (abortController) {
        abortController.abort();
        abortController = null;
      }
    }
    
    // Handle send button click (stop if streaming)
    chatSendBtn.addEventListener('click', (e) => {
      if (isStreaming) {
        e.preventDefault();
        stopStreaming();
      }
      // If not streaming, let the form submit naturally
    });

    // Handle chat form submission with SSE streaming
    chatForm.addEventListener('submit', async (e) => {
      e.preventDefault();
      
      const message = chatInput.value.trim();
      if (!message || isStreaming) return;
      
      // Switch to stop button mode
      setStreamingMode(true);
      abortController = new AbortController();
      
      // Add user message
      addMessage(message, true);
      chatInput.value = '';
      
      // Show typing indicator
      showTypingIndicator();
      
      let streamingContent = null;
      
      try {
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message, session_id: sessionId }),
          signal: abortController.signal
        });
        
        if (!response.ok) {
          hideTypingIndicator();
          addMessage('Sorry, something went wrong. Please try again.', false);
          setStreamingMode(false);
          chatInput.focus();
          return;
        }

        // Read the SSE stream
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let firstChunkReceived = false;
        
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          
          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data === '[DONE]') continue;
              
              try {
                const json = JSON.parse(data);
                if (json.error) {
                  hideTypingIndicator();
                  addMessage('Sorry, something went wrong. Please try again.', false);
                } else if (json.content) {
                  // Hide typing indicator and create message on first content
                  if (!firstChunkReceived) {
                    hideTypingIndicator();
                    streamingContent = createStreamingMessage();
                    firstChunkReceived = true;
                  }
                  // Accumulate markdown text and render as HTML
                  const currentText = streamingContent.dataset.markdown || '';
                  const newText = currentText + json.content;
                  streamingContent.dataset.markdown = newText;
                  streamingContent.innerHTML = markdownToHtml(newText);
                  chatMessages.scrollTop = chatMessages.scrollHeight;
                }
              } catch (e) {
                // Skip invalid JSON
              }
            }
          }
        }
        
        // Remove the id so future streams don't conflict
        if (streamingContent) {
          streamingContent.removeAttribute('id');
        }
        
        // Switch back to send button
        setStreamingMode(false);
        chatInput.focus();
        
      } catch (error) {
        hideTypingIndicator();
        
        // Check if this was an abort (user clicked stop)
        if (error.name === 'AbortError') {
          // Remove the id so future streams don't conflict
          if (streamingContent) {
            streamingContent.removeAttribute('id');
          }
        } else {
          addMessage('Sorry, I couldn\'t connect to the server.', false);
        }
        
        // Switch back to send button
        setStreamingMode(false);
        chatInput.focus();
      }
      
      abortController = null;
    });

    // Handle example question button clicks
    document.querySelectorAll('.example-question-btn').forEach(btn => {
      btn.addEventListener('click', () => {
        const question = btn.dataset.question;
        if (question) {
          // Set the input value and submit
          chatInput.value = question;
          chatForm.dispatchEvent(new Event('submit', { cancelable: true, bubbles: true }));
        }
      });
    });

    // =====================================================
    // VOICE CALL FUNCTIONALITY
    // =====================================================
    
    // Voice WebSocket and audio state
    let voiceWebSocket = null;
    let audioContext = null;
    let micStream = null;
    let micProcessor = null;
    let isPlayingAudio = false;
    let audioInterrupted = false;  // Flag to ignore audio after interrupt
    let voiceState = 'idle'; // idle, listening, processing, speaking
    
    // Get WebSocket URL based on current page URL
    function getVoiceWebSocketUrl() {
      const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      return `${protocol}//${window.location.host}/api/voice/stream`;
    }
    
    // Initialize AudioContext for playback (at playback sample rate)
    let playbackContext = null;
    function initPlaybackContext() {
      if (!playbackContext) {
        playbackContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000
        });
      }
      if (playbackContext.state === 'suspended') {
        playbackContext.resume();
      }
      return playbackContext;
    }
    
    // Convert Linear16 PCM bytes to AudioBuffer
    function pcmToAudioBuffer(pcmData) {
      const ctx = initPlaybackContext();
      
      // Linear16 PCM is 16-bit signed integers
      const int16Array = new Int16Array(pcmData.buffer, pcmData.byteOffset, pcmData.byteLength / 2);
      const float32Array = new Float32Array(int16Array.length);
      
      // Convert to float32 (-1.0 to 1.0)
      for (let i = 0; i < int16Array.length; i++) {
        float32Array[i] = int16Array[i] / 32768.0;
      }
      
      // Create AudioBuffer
      const audioBuffer = ctx.createBuffer(1, float32Array.length, 16000);
      audioBuffer.getChannelData(0).set(float32Array);
      
      return audioBuffer;
    }
    
    // Seamless audio playback using scheduled timing
    let nextPlayTime = 0;
    let activeSources = [];  // Track ALL active audio sources
    
    function queueAudio(audioBuffer) {
      const ctx = initPlaybackContext();
      
      // If this is the first chunk or we're behind, start from now
      const currentTime = ctx.currentTime;
      if (nextPlayTime < currentTime) {
        nextPlayTime = currentTime;
      }
      
      // Create and schedule the source
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(ctx.destination);
      
      // Schedule to play at the precise time
      source.start(nextPlayTime);
      
      // Track this source for stopping during interruption
      activeSources.push(source);
      isPlayingAudio = true;
      
      // Update next play time to be right after this buffer ends
      nextPlayTime += audioBuffer.duration;
      
      // Clean up when this source ends
      source.onended = () => {
        // Remove from active sources
        const idx = activeSources.indexOf(source);
        if (idx > -1) {
          activeSources.splice(idx, 1);
        }
        // Mark as not playing if no more sources
        if (activeSources.length === 0) {
          isPlayingAudio = false;
        }
      };
    }
    
    // Stop audio playback (for interruption)
    function stopAudioPlayback() {
      console.log(`Stopping audio playback (${activeSources.length} active sources)`);
      const ctx = initPlaybackContext();
      
      // Stop ALL active sources
      for (const source of activeSources) {
        try {
          source.stop();
        } catch (e) {
          // Source might already be stopped
        }
      }
      activeSources = [];
      
      // Reset timing
      nextPlayTime = ctx.currentTime;
      isPlayingAudio = false;
    }
    
    // Convert Float32 samples to Int16 (Linear16 PCM)
    function float32ToInt16(float32Array) {
      const int16Array = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        // Clamp to -1.0 to 1.0 range and convert to 16-bit
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return int16Array;
    }
    
    // Downsample audio from source rate to target rate
    function downsample(samples, sourceSampleRate, targetSampleRate) {
      if (sourceSampleRate === targetSampleRate) {
        return samples;
      }
      const ratio = sourceSampleRate / targetSampleRate;
      const newLength = Math.round(samples.length / ratio);
      const result = new Float32Array(newLength);
      for (let i = 0; i < newLength; i++) {
        const srcIndex = Math.round(i * ratio);
        result[i] = samples[srcIndex];
      }
      return result;
    }
    
    // Start microphone recording with raw PCM output
    async function startRecording() {
      // Check if already recording
      if (micStream) {
        console.log('Already recording');
        return;
      }
      
      // Check if we're in a secure context (HTTPS or localhost)
      // Note: Chrome treats "localhost" as secure but NOT "127.0.0.1"
      if (!window.isSecureContext) {
        const currentHost = window.location.hostname;
        if (currentHost === '127.0.0.1') {
          throw new Error('Microphone requires a secure context. Please access this site via http://localhost:3001 instead of http://127.0.0.1:3001');
        }
        throw new Error('Microphone access requires HTTPS. Please access this site via HTTPS or http://localhost:3001');
      }
      
      // Check if getUserMedia is available
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('Microphone API not available. Please use a modern browser (Chrome, Firefox, Edge, Safari).');
      }
      
      // Pre-check permission state if available (for better error messages)
      if (navigator.permissions && navigator.permissions.query) {
        try {
          const result = await navigator.permissions.query({ name: 'microphone' });
          console.log('Microphone permission state:', result.state);
          if (result.state === 'denied') {
            throw new Error('Microphone access was blocked. Please click the lock/site-settings icon in your browser\'s address bar, allow microphone access, then refresh the page.');
          }
        } catch (e) {
          // Some browsers don't support querying microphone permission - that's ok
          console.log('Could not query permission state (this is normal):', e.message);
        }
      }
      
      try {
        console.log('Requesting microphone access...');
        micStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          } 
        });
        console.log('Microphone access granted');
        
        // Create audio context for capture
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaStreamSource(micStream);
        
        // Use ScriptProcessorNode to get raw PCM (deprecated but widely supported)
        // Buffer size of 4096 gives ~85ms at 48kHz which we downsample to 16kHz
        const bufferSize = 4096;
        micProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        
        micProcessor.onaudioprocess = (event) => {
          if (voiceWebSocket && voiceWebSocket.readyState === WebSocket.OPEN) {
            const inputData = event.inputBuffer.getChannelData(0);
            
            // Downsample to 16kHz if needed
            const downsampled = downsample(inputData, audioContext.sampleRate, 16000);
            
            // Convert to Linear16 PCM
            const pcmData = float32ToInt16(downsampled);
            
            // Send as binary
            voiceWebSocket.send(pcmData.buffer);
          }
        };
        
        // Connect: source -> processor -> destination (required for processor to work)
        source.connect(micProcessor);
        micProcessor.connect(audioContext.destination);
        
        console.log('Recording started (raw PCM at 16kHz)');
        
      } catch (error) {
        console.error('Error starting recording:', error);
        
        // Provide specific error messages
        if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
          throw new Error('Microphone permission denied. Please allow microphone access in your browser settings and try again.');
        } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
          throw new Error('No microphone found. Please connect a microphone and try again.');
        } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
          throw new Error('Microphone is already in use by another application. Please close other apps using the microphone.');
        } else if (error.name === 'OverconstrainedError') {
          throw new Error('Microphone does not support the required settings. Please try a different microphone.');
        } else {
          throw new Error(`Microphone error: ${error.message || error.name}`);
        }
      }
    }
    
    // Stop microphone recording
    function stopRecording() {
      if (micProcessor) {
        micProcessor.disconnect();
        micProcessor = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      if (micStream) {
        micStream.getTracks().forEach(track => track.stop());
        micStream = null;
      }
      console.log('Recording stopped');
    }
    
    // Connect to voice WebSocket
    function connectVoiceWebSocket() {
      return new Promise((resolve, reject) => {
        const url = getVoiceWebSocketUrl();
        console.log('Connecting to voice WebSocket:', url);
        
        let resolved = false;
        let timeoutId = null;
        
        voiceWebSocket = new WebSocket(url);
        
        voiceWebSocket.onopen = () => {
          console.log('Voice WebSocket connected');
          // Send start message with session ID
          voiceWebSocket.send(JSON.stringify({
            type: 'start',
            session_id: sessionId
          }));
        };
        
        voiceWebSocket.onmessage = async (event) => {
          if (event.data instanceof Blob) {
            // Binary audio data from TTS
            // Ignore audio if we've been interrupted
            if (audioInterrupted) {
              return;
            }
            const arrayBuffer = await event.data.arrayBuffer();
            const audioBuffer = pcmToAudioBuffer(new Uint8Array(arrayBuffer));
            queueAudio(audioBuffer);
          } else {
            // JSON control message
            try {
              const data = JSON.parse(event.data);
              handleVoiceMessage(data);
              
              if (data.type === 'ready' && !resolved) {
                resolved = true;
                if (timeoutId) clearTimeout(timeoutId);
                resolve();
              } else if (data.type === 'error') {
                if (!resolved) {
                  resolved = true;
                  if (timeoutId) clearTimeout(timeoutId);
                  reject(new Error(data.message || 'Server error'));
                }
              }
            } catch (e) {
              console.error('Error parsing voice message:', e);
            }
          }
        };
        
        voiceWebSocket.onerror = (error) => {
          console.error('Voice WebSocket error:', error);
          if (!resolved) {
            resolved = true;
            if (timeoutId) clearTimeout(timeoutId);
            reject(new Error('WebSocket connection failed. Is the server running?'));
          }
        };
        
        voiceWebSocket.onclose = (event) => {
          console.log('Voice WebSocket closed', event.code, event.reason);
          voiceWebSocket = null;
          if (!resolved) {
            resolved = true;
            if (timeoutId) clearTimeout(timeoutId);
            reject(new Error('WebSocket connection closed. Please check if the server is running.'));
          }
        };
        
        // Timeout after 10 seconds
        timeoutId = setTimeout(() => {
          if (!resolved && voiceWebSocket && voiceWebSocket.readyState !== WebSocket.OPEN) {
            resolved = true;
            voiceWebSocket.close();
            reject(new Error('Connection timeout - server did not respond'));
          }
        }, 10000);
      });
    }
    
    // Handle voice WebSocket messages
    function handleVoiceMessage(data) {
      switch (data.type) {
        case 'ready':
          console.log('Voice session ready');
          break;
          
        case 'state':
          const prevState = voiceState;
          voiceState = data.state;
          console.log('Voice state:', prevState, 'â†’', voiceState);
          updateVoiceUI();
          
          // When processing starts, clear any leftover audio from previous response
          if (voiceState === 'processing') {
            stopAudioPlayback();
            audioInterrupted = false;
          }
          break;
        
        case 'interrupt':
          // Explicit interrupt signal - stop audio immediately
          console.log('Interrupt signal received - stopping audio');
          audioInterrupted = true;  // Ignore any audio still in flight
          stopAudioPlayback();
          break;
          
        case 'transcript':
          console.log(`Transcript (${data.is_final ? 'final' : 'interim'}):`, data.text);
          // Could show transcript in UI if desired
          break;
          
        case 'response':
          console.log('Response text:', data.text);
          // Could show response text in UI if desired
          break;
          
        case 'error':
          console.error('Voice error:', data.message);
          break;
      }
    }
    
    // Update UI based on voice state
    function updateVoiceUI() {
      // Could add visual indicators for listening/processing/speaking states
      // For now, just log
    }
    
    // Disconnect voice WebSocket
    function disconnectVoiceWebSocket() {
      if (voiceWebSocket) {
        voiceWebSocket.send(JSON.stringify({ type: 'stop' }));
        voiceWebSocket.close();
        voiceWebSocket = null;
      }
    }
    
    // Start voice call
    async function startVoiceCall() {
      try {
        // Initialize playback context (needs user gesture)
        initPlaybackContext();
        
        // FIRST: Request microphone permission before connecting WebSocket
        // This ensures we fail fast if permission is denied
        console.log('Requesting microphone permission...');
        await startRecording();
        console.log('Microphone access granted');
        
        // THEN: Connect to voice WebSocket
        console.log('Connecting to voice server...');
        await connectVoiceWebSocket();
        
        console.log('Voice call started');
        return true;
      } catch (error) {
        console.error('Failed to start voice call:', error);
        
        // Cleanup on failure
        stopRecording();
        disconnectVoiceWebSocket();
        
        // Determine specific error type for user-friendly message
        let errorMessage = error.message || 'Unknown error';
        
        // If it's already a user-friendly message, use it as-is
        if (!errorMessage.includes('Failed to start')) {
          errorMessage = 'Failed to start voice call: ' + errorMessage;
        }
        
        throw new Error(errorMessage);
      }
    }
    
    // End voice call
    function endVoiceCall() {
      stopRecording();
      stopAudioPlayback();
      disconnectVoiceWebSocket();
      voiceState = 'idle';
      console.log('Voice call ended');
    }

    // Floating call button
    callFab.addEventListener('click', async () => {
      const callIcon = callFab.querySelector('.call-icon');
      const endCallIcon = callFab.querySelector('.end-call-icon');
      const tooltip = callFab.querySelector('.fab-tooltip');
      
      if (!isInCall) {
        // Start call - trigger call answer flow
        isInCall = true;
        
        // Hide other FABs
        downloadFab.classList.add('fab-hidden');
        chatFab.classList.add('fab-hidden');
        
        // Switch to X icon
        callIcon.style.display = 'none';
        endCallIcon.style.display = 'block';
        tooltip.textContent = 'End Call';
        callFab.classList.add('call-active');
        
        // Trigger call answer video
        callEndVideo.classList.remove('active');
        callEndVideo.pause();
        callAnswerVideo.currentTime = 0;
        callAnswerVideo.classList.add('active');
        callAnswerVideo.play();
        
        // Start voice call
        try {
          await startVoiceCall();
        } catch (error) {
          // Revert UI on failure
          isInCall = false;
          downloadFab.classList.remove('fab-hidden');
          chatFab.classList.remove('fab-hidden');
          callIcon.style.display = 'block';
          endCallIcon.style.display = 'none';
          tooltip.textContent = 'Start Call';
          callFab.classList.remove('call-active');
          hideAllOverlays();
          alert(error.message || 'Failed to start voice call. Please check microphone permissions.');
        }
        
      } else {
        // End call - trigger call end flow
        isInCall = false;
        
        // End voice call
        endVoiceCall();
        
        // Show other FABs
        downloadFab.classList.remove('fab-hidden');
        chatFab.classList.remove('fab-hidden');
        
        // Switch back to phone icon
        callIcon.style.display = 'block';
        endCallIcon.style.display = 'none';
        tooltip.textContent = 'Start Call';
        callFab.classList.remove('call-active');
        
        // Trigger call end video
        callEndVideo.currentTime = 0;
        callEndVideo.classList.add('active');
        callEndVideo.play().then(() => {
          setTimeout(() => {
            callAnswerVideo.classList.remove('active');
            callAnswerVideo.pause();
          }, 200);
        });
        
        callEndVideo.addEventListener('ended', function hideCallEnd() {
          callEndVideo.classList.remove('active');
          callEndVideo.removeEventListener('ended', hideCallEnd);
        }, { once: true });
      }
    });
  </script>

</body>
</html>