<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Ankit Badatala - Portfolio</title>
  <link rel="stylesheet" href="/styles.css" />
  
  <!-- Preload critical assets for faster rendering -->
  <link rel="preload" href="/objects/marble-floor.webp" as="image" />
  <link rel="preload" href="/objects/videos/marble-typing-astra.mp4" as="video" type="video/mp4" />
</head>
<body>

  <div class="frame">
    <!-- Background typing video - always playing -->
    <video class="vid vid-bg" autoplay muted loop playsinline preload="auto">
      <source src="/objects/videos/marble-typing-astra.mp4" type="video/mp4" />
    </video>
    
    <!-- Overlay videos - shown on top when needed -->
    <video class="vid vid-overlay" id="call-answer-vid" muted playsinline>
      <source src="/objects/videos/call-answer.mp4" type="video/mp4" />
    </video>
    
    <video class="vid vid-overlay" id="call-end-vid" muted playsinline>
      <source src="/objects/videos/call-end.mp4" type="video/mp4" />
    </video>
  </div>

  <!-- Floating Action Buttons -->
  <div class="fab-container">
    <button class="fab download-fab" id="download-fab" aria-label="Download Resume">
      <svg width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M19 9h-4V3H9v6H5l7 7 7-7zM5 18v2h14v-2H5z"/>
      </svg>
      <span class="fab-tooltip">Download Resume</span>
    </button>
    <button class="fab chat-fab" id="chat-fab" aria-label="Open chat">
      <svg width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z"/>
      </svg>
      <span class="fab-tooltip">Open Chat</span>
    </button>
    <button class="fab call-fab" id="call-fab" aria-label="Start Call">
      <svg class="call-icon" width="28" height="28" viewBox="0 0 24 24" fill="currentColor">
        <path d="M20.01 15.38c-1.23 0-2.42-.2-3.53-.56-.35-.12-.74-.03-1.01.24l-1.57 1.97c-2.83-1.35-5.48-3.9-6.89-6.83l1.95-1.66c.27-.28.35-.67.24-1.02-.37-1.11-.56-2.3-.56-3.53 0-.54-.45-.99-.99-.99H4.19C3.65 3 3 3.24 3 3.99 3 13.28 10.73 21 20.01 21c.71 0 .99-.63.99-1.18v-3.45c0-.54-.45-.99-.99-.99z"/>
      </svg>
      <svg class="end-call-icon" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" style="display: none;">
        <path d="M18 6L6 18M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"/>
      </svg>
      <span class="fab-tooltip">Start Call</span>
      <span class="rate-limit-badge" id="rate-limit-badge" style="display: none;">!</span>
    </button>
  </div>
  
  <!-- Rate limit warning toast -->
  <div class="rate-limit-toast" id="rate-limit-toast">
    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
      <circle cx="12" cy="12" r="10"/>
      <path d="M12 8v4M12 16h.01"/>
    </svg>
    <span>Voice busy - please wait a moment</span>
  </div>

  <!-- Chat Window - degree-guru style -->
  <div class="chat-panel" id="chat-panel">
    <!-- Close button -->
    <button class="chat-close-btn" id="chat-close-btn" aria-label="Close chat">
      <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M18 6L6 18M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"/>
      </svg>
    </button>
    <div class="chat-messages" id="chat-messages">
      <!-- Welcome message -->
      <div class="chat-message ai-message">
        <div class="message-avatar ai-avatar">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
          </svg>
        </div>
        <div class="message-content">
          <strong>Welcome to Ankit's Portfolio</strong><br><br>
          Ankit built me to help answer any questions you might have about his career, experience, or projects.
          <div class="example-questions">
            <button class="example-question-btn" data-question="What should I ask you about if I only have 5 minutes?">What should I ask you about if I only have 5 minutes?</button>
            <button class="example-question-btn" data-question="What are Ankit's technical skills?">What are Ankit's technical skills?</button>
            <button class="example-question-btn" data-question="Tell me about his leadership style">Tell me about his leadership style.</button>
            <button class="example-question-btn" data-question="Is Ankit a strong fit for a Voice AI Engineer role?">Is Ankit a strong fit for a Voice AI Engineer role?</button>
            <button class="example-question-btn" data-question="How did he build the image to dashboard computer vision pipeline">How did he build the image to dashboard computer vision pipeline?</button>
          </div>
        </div>
      </div>
    </div>
    
    <div class="chat-input-area">
      <form class="chat-form" id="chat-form">
        <input 
          type="text" 
          class="chat-input" 
          id="chat-input" 
          placeholder="Your question..." 
          autocomplete="off"
          required
        />
        <button type="submit" class="chat-send-btn" id="chat-send-btn">
          <svg class="send-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M22 2L11 13" stroke-linecap="round" stroke-linejoin="round"/>
            <path d="M22 2L15 22L11 13L2 9L22 2Z" stroke-linecap="round" stroke-linejoin="round"/>
          </svg>
          <svg class="stop-icon" width="18" height="18" viewBox="0 0 24 24" fill="currentColor" style="display: none;">
            <rect x="4" y="4" width="16" height="16" rx="2"/>
          </svg>
        </button>
      </form>
    </div>
  </div>


  <!-- Marked.js for markdown rendering -->
  <script src="https://cdn.jsdelivr.net/npm/marked@12.0.0/marked.min.js"></script>
  
  <script>
    const frame = document.querySelector('.frame');
    const bgVideo = document.querySelector('.vid-bg');
    const callAnswerVideo = document.getElementById('call-answer-vid');
    const callEndVideo = document.getElementById('call-end-vid');
    
    // Chat elements
    const chatPanel = document.getElementById('chat-panel');
    const chatForm = document.getElementById('chat-form');
    const chatInput = document.getElementById('chat-input');
    const chatSendBtn = document.getElementById('chat-send-btn');
    const sendIcon = chatSendBtn.querySelector('.send-icon');
    const stopIcon = chatSendBtn.querySelector('.stop-icon');
    const chatMessages = document.getElementById('chat-messages');
    const chatFab = document.getElementById('chat-fab');
    const chatCloseBtn = document.getElementById('chat-close-btn');
    const downloadFab = document.getElementById('download-fab');
    const callFab = document.getElementById('call-fab');
    
    // Call state
    let isInCall = false;
    
    // Chat streaming state
    let isStreaming = false;
    let abortController = null;
    
    // Toggle button between send and stop
    function setStreamingMode(streaming) {
      isStreaming = streaming;
      sendIcon.style.display = streaming ? 'none' : 'block';
      stopIcon.style.display = streaming ? 'block' : 'none';
    }
    
    // Session management for chat history
    let sessionId = null;
    
    // Generate a new session ID
    function generateSessionId() {
      return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
        const r = Math.random() * 16 | 0;
        const v = c === 'x' ? r : (r & 0x3 | 0x8);
        return v.toString(16);
      });
    }
    
    // Initialize session on page load
    sessionId = generateSessionId();
    console.log('Chat session initialized:', sessionId);


    // Hide all overlay videos
    function hideAllOverlays() {
      callAnswerVideo.classList.remove('active');
      callEndVideo.classList.remove('active');
      callAnswerVideo.pause();
      callEndVideo.pause();
    }

    function downloadResume() {
      const link = document.createElement('a');
      link.href = 'objects/Ankits_Badatala_Resume.pdf';
      link.download = 'Ankits_Badatala_Resume.pdf';
      document.body.appendChild(link);
      link.click();
      document.body.removeChild(link);
    }

    downloadFab.addEventListener('click', downloadResume);

    // Toggle chat panel
    function openChat() {
      frame.classList.add('chat-open');
      chatPanel.classList.add('open');
      chatInput.focus();
    }

    function closeChat() {
      frame.classList.remove('chat-open');
      chatPanel.classList.remove('open');
    }

    // Floating chat button
    chatFab.addEventListener('click', () => {
      openChat();
    });

    // Close button in chat panel
    chatCloseBtn.addEventListener('click', () => {
      closeChat();
    });

    // SVG icons for avatars
    const aiAvatarSVG = `<svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
      <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
    </svg>`;
    
    const userAvatarSVG = `<svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
      <path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"/>
    </svg>`;

    // Add message to chat
    function addMessage(content, isUser = false) {
      const messageDiv = document.createElement('div');
      messageDiv.className = `chat-message ${isUser ? 'user-message' : 'ai-message'}`;
      
      messageDiv.innerHTML = `
        <div class="message-avatar ${isUser ? 'user-avatar' : 'ai-avatar'}">${isUser ? userAvatarSVG : aiAvatarSVG}</div>
        <div class="message-content">${content}</div>
      `;
      
      chatMessages.appendChild(messageDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
      
      return messageDiv;
    }

    // Simple markdown to HTML converter (handles basic formatting)
    function markdownToHtml(text) {
      // Convert markdown to HTML
      if (typeof marked !== 'undefined') {
        return marked.parse(text);
      }
      // Fallback: basic formatting if marked.js not loaded
      return text
        .replace(/\n/g, '<br>')
        .replace(/^- (.+)$/gm, '<li>$1</li>')
        .replace(/(<li>.*<\/li>)/s, '<ul>$1</ul>')
        .replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>')
        .replace(/\*(.+?)\*/g, '<em>$1</em>');
    }

    // Create an empty AI message for streaming
    function createStreamingMessage() {
      const messageDiv = document.createElement('div');
      messageDiv.className = 'chat-message ai-message';
      messageDiv.innerHTML = `
        <div class="message-avatar ai-avatar">${aiAvatarSVG}</div>
        <div class="message-content" id="streaming-content"></div>
      `;
      chatMessages.appendChild(messageDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
      return messageDiv.querySelector('#streaming-content');
    }

    // Show typing indicator - degree-guru style bouncing dots
    function showTypingIndicator() {
      const typingDiv = document.createElement('div');
      typingDiv.className = 'message-loading';
      typingDiv.id = 'typing-indicator';
      
      typingDiv.innerHTML = `
        <div class="message-avatar ai-avatar">${aiAvatarSVG}</div>
        <div class="loading-dots">
          <span></span>
          <span></span>
          <span></span>
        </div>
      `;
      
      chatMessages.appendChild(typingDiv);
      chatMessages.scrollTop = chatMessages.scrollHeight;
    }

    function hideTypingIndicator() {
      const typingDiv = document.getElementById('typing-indicator');
      if (typingDiv) {
        typingDiv.remove();
      }
    }

    // Handle stop button click
    function stopStreaming() {
      if (abortController) {
        abortController.abort();
        abortController = null;
      }
    }
    
    // Handle send button click (stop if streaming)
    chatSendBtn.addEventListener('click', (e) => {
      if (isStreaming) {
        e.preventDefault();
        stopStreaming();
      }
      // If not streaming, let the form submit naturally
    });

    // Handle chat form submission with SSE streaming
    chatForm.addEventListener('submit', async (e) => {
      e.preventDefault();
      
      const message = chatInput.value.trim();
      if (!message || isStreaming) return;
      
      // Switch to stop button mode
      setStreamingMode(true);
      abortController = new AbortController();
      
      // Add user message
      addMessage(message, true);
      chatInput.value = '';
      
      // Show typing indicator
      showTypingIndicator();
      
      let streamingContent = null;
      
      try {
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message, session_id: sessionId }),
          signal: abortController.signal
        });
        
        if (!response.ok) {
          hideTypingIndicator();
          addMessage('Sorry, something went wrong. Please try again.', false);
          setStreamingMode(false);
          chatInput.focus();
          return;
        }

        // Read the SSE stream
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let firstChunkReceived = false;
        
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          
          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data === '[DONE]') continue;
              
              try {
                const json = JSON.parse(data);
                if (json.error) {
                  hideTypingIndicator();
                  addMessage('Sorry, something went wrong. Please try again.', false);
                } else if (json.content) {
                  // Hide typing indicator and create message on first content
                  if (!firstChunkReceived) {
                    hideTypingIndicator();
                    streamingContent = createStreamingMessage();
                    firstChunkReceived = true;
                  }
                  // Accumulate markdown text and render as HTML
                  const currentText = streamingContent.dataset.markdown || '';
                  const newText = currentText + json.content;
                  streamingContent.dataset.markdown = newText;
                  streamingContent.innerHTML = markdownToHtml(newText);
                  chatMessages.scrollTop = chatMessages.scrollHeight;
                }
              } catch (e) {
                // Skip invalid JSON
              }
            }
          }
        }
        
        // Remove the id so future streams don't conflict
        if (streamingContent) {
          streamingContent.removeAttribute('id');
        }
        
        // Switch back to send button
        setStreamingMode(false);
        chatInput.focus();
        
      } catch (error) {
        hideTypingIndicator();
        
        // Check if this was an abort (user clicked stop)
        if (error.name === 'AbortError') {
          // Remove the id so future streams don't conflict
          if (streamingContent) {
            streamingContent.removeAttribute('id');
          }
        } else {
          addMessage('Sorry, I couldn\'t connect to the server.', false);
        }
        
        // Switch back to send button
        setStreamingMode(false);
        chatInput.focus();
      }
      
      abortController = null;
    });

    // Handle example question button clicks
    document.querySelectorAll('.example-question-btn').forEach(btn => {
      btn.addEventListener('click', () => {
        const question = btn.dataset.question;
        if (question) {
          // Set the input value and submit
          chatInput.value = question;
          chatForm.dispatchEvent(new Event('submit', { cancelable: true, bubbles: true }));
        }
      });
    });

    // =====================================================
    // VOICE CALL FUNCTIONALITY (Client-Side Deepgram)
    // =====================================================
    
    // Voice state
    let deepgramToken = null;
    let sttSocket = null;  // Deepgram Flux STT WebSocket
    let ttsSocket = null;  // Deepgram Aura TTS WebSocket
    let audioContext = null;
    let micStream = null;
    let micProcessor = null;
    let isPlayingAudio = false;
    let audioInterrupted = false;
    let voiceState = 'idle'; // idle, listening, processing, speaking
    let isProcessingResponse = false;  // Prevent duplicate processing
    
    // Connection tracking for debugging and rate limit prevention
    let connectionStats = {
      sttConnections: 0,
      ttsConnections: 0,
      sttConnectionsCreated: 0,
      ttsConnectionsCreated: 0,
      lastSTTConnect: null,
      lastTTSConnect: null
    };
    
    // Get Deepgram token from server (keeps API key server-side)
    async function getDeepgramToken() {
      const response = await fetch('/api/deepgram-token');
      if (!response.ok) {
        throw new Error('Failed to get Deepgram token');
      }
      const data = await response.json();
      return data.key;
    }
    
    // Connect to Deepgram Flux STT
    async function connectSTT(token) {
      // Safeguard: Close existing connection if any
      if (sttSocket) {
        const state = sttSocket.readyState;
        console.warn(`STT connection already exists (state: ${state}), closing before creating new one`);
        try {
          sttSocket.close();
        } catch (e) {}
        sttSocket = null;
      }
      
      return new Promise((resolve, reject) => {
        const params = new URLSearchParams({
          model: 'flux-general-en',
          encoding: 'linear16',
          sample_rate: '16000',
          eot_threshold: '0.7',
          eager_eot_threshold: '0.4',
          eot_timeout_ms: '6000'
        });
        
        const url = `wss://api.deepgram.com/v2/listen?${params}`;
        console.log('Connecting to Deepgram STT...');
        connectionStats.sttConnectionsCreated++;
        connectionStats.lastSTTConnect = Date.now();
        
        sttSocket = new WebSocket(url, ['token', token]);
        connectionStats.sttConnections++;
        
        sttSocket.onopen = () => {
          console.log(`Deepgram STT connected (total STT connections created: ${connectionStats.sttConnectionsCreated})`);
          voiceState = 'listening';
          resolve();
        };
        
        sttSocket.onmessage = (event) => {
          try {
            const data = JSON.parse(event.data);
            // Log all message types for debugging
            console.log(`STT message: ${data.type}`);
            handleSTTMessage(data);
          } catch (e) {
            console.error('Error parsing STT message:', e);
          }
        };
        
        sttSocket.onerror = (error) => {
          console.error('STT WebSocket error:', error);
          connectionStats.sttConnections = Math.max(0, connectionStats.sttConnections - 1);
          reject(new Error('Failed to connect to speech recognition'));
        };
        
        sttSocket.onclose = (event) => {
          console.log(`STT WebSocket closed (code: ${event.code}, total STT connections created: ${connectionStats.sttConnectionsCreated})`);
          connectionStats.sttConnections = Math.max(0, connectionStats.sttConnections - 1);
          sttSocket = null;
        };
        
        // Timeout
        setTimeout(() => {
          if (sttSocket && sttSocket.readyState !== WebSocket.OPEN) {
            sttSocket.close();
            connectionStats.sttConnections = Math.max(0, connectionStats.sttConnections - 1);
            reject(new Error('STT connection timeout'));
          }
        }, 10000);
      });
    }
    
    // Handle Deepgram STT messages
    function handleSTTMessage(data) {
      const msgType = data.type;
      
      // Log all STT events for debugging
      if (msgType === 'TurnInfo') {
        console.log(`STT TurnInfo: ${data.event} - "${(data.transcript || '').substring(0, 50)}..."`);
      }
      
      if (msgType === 'TurnInfo') {
        const event = data.event;
        const transcript = data.transcript || '';
        
        if (event === 'EndOfTurn' && transcript.trim()) {
          console.log('End of turn:', transcript);
          console.log(`State check: isProcessingResponse=${isProcessingResponse}, voiceState=${voiceState}`);
          
          // Prevent duplicate processing
          if (isProcessingResponse) {
            console.log('Already processing, ignoring duplicate EndOfTurn');
            return;
          }
          
          voiceState = 'processing';
          stopAudioPlayback();
          audioInterrupted = false;
          processAndSpeak(transcript);
        }
        else if (event === 'StartOfTurn' || event === 'SpeechStarted') {
          // User started speaking - interrupt if AI is speaking
          if (voiceState === 'speaking') {
            console.log('Interruption detected');
            audioInterrupted = true;
            stopAudioPlayback();
            closeTTS();
            voiceState = 'listening';
          }
        }
      }
    }
    
    // TTS flush tracking (like backend _pending_flushes and _flush_event)
    let ttsPendingFlushes = 0;
    let ttsFlushResolvers = [];
    
    // Track TTS connection timing to avoid rate limits
    let lastTTSConnectTime = 0;
    const TTS_MIN_CONNECT_INTERVAL = 2000; // Minimum 2 seconds between new connections
    let voiceRateLimited = false; // Track if we're currently rate limited
    
    // Concurrency management (Deepgram guidance: only open new when you have "slots")
    const MAX_CONCURRENT_TTS = 1; // Only allow 1 TTS connection at a time
    
    function hasTTSSlotAvailable() {
      // Check if we have a slot: no active connection OR existing connection is OPEN and reusable
      const activeConnections = connectionStats.ttsConnections;
      const hasOpenSocket = ttsSocket && ttsSocket.readyState === WebSocket.OPEN;
      
      // Slot is available if: no active connections, OR we have an open socket we can reuse
      return activeConnections < MAX_CONCURRENT_TTS || hasOpenSocket;
    }
    
    async function waitForTTSSlot(timeout = 10000) {
      const startTime = Date.now();
      while (!hasTTSSlotAvailable()) {
        if (Date.now() - startTime > timeout) {
          throw new Error('Timeout waiting for TTS slot');
        }
        console.log(`Waiting for TTS slot (active: ${connectionStats.ttsConnections}/${MAX_CONCURRENT_TTS})...`);
        await new Promise(r => setTimeout(r, 500));
      }
    }
    
    // Rate limit warning UI
    const rateLimitBadge = document.getElementById('rate-limit-badge');
    const rateLimitToast = document.getElementById('rate-limit-toast');
    
    function showRateLimitWarning() {
      console.log('Showing rate limit warning');
      if (rateLimitBadge) rateLimitBadge.style.display = 'flex';
      if (rateLimitToast) rateLimitToast.classList.add('visible');
      
      // Auto-hide toast after 5 seconds (badge stays until resolved)
      setTimeout(() => {
        if (rateLimitToast) rateLimitToast.classList.remove('visible');
      }, 5000);
    }
    
    function hideRateLimitWarning() {
      console.log('Hiding rate limit warning');
      if (rateLimitBadge) rateLimitBadge.style.display = 'none';
      if (rateLimitToast) rateLimitToast.classList.remove('visible');
    }
    
    // Single TTS connection attempt (internal)
    function tryConnectTTS(token) {
      // Safeguard: Close existing connection PROPERLY before creating new one
      if (ttsSocket) {
        const state = ttsSocket.readyState;
        console.warn(`TTS connection already exists (state: ${state}), closing before creating new one`);
        try {
          // Use proper close code so Deepgram registers the disconnect
          ttsSocket.close(1000, 'Creating new connection');
        } catch (e) {}
        ttsSocket = null;
        connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
      }
      
      return new Promise((resolve, reject) => {
        const params = new URLSearchParams({
          model: 'aura-2-odysseus-en',
          encoding: 'linear16',
          sample_rate: '16000'
        });
        
        const url = `wss://api.deepgram.com/v1/speak?${params}`;
        console.log('Connecting to Deepgram TTS...');
        lastTTSConnectTime = Date.now();
        connectionStats.ttsConnectionsCreated++;
        connectionStats.lastTTSConnect = Date.now();
        
        ttsSocket = new WebSocket(url, ['token', token]);
        connectionStats.ttsConnections++;
        ttsPendingFlushes = 0;
        ttsFlushResolvers = [];
        
        let settled = false;
        
        ttsSocket.onopen = () => {
          if (!settled) {
            settled = true;
            console.log(`Deepgram TTS connected (total TTS connections created: ${connectionStats.ttsConnectionsCreated})`);
            resolve();
          }
        };
        
        ttsSocket.onmessage = async (event) => {
          if (event.data instanceof Blob) {
            // Audio data
            if (audioInterrupted) return;
            
            const arrayBuffer = await event.data.arrayBuffer();
            const audioBuffer = pcmToAudioBuffer(new Uint8Array(arrayBuffer));
            queueAudio(audioBuffer);
          } else {
            // Control message
            try {
              const data = JSON.parse(event.data);
              if (data.type === 'Flushed') {
                ttsPendingFlushes = Math.max(0, ttsPendingFlushes - 1);
                console.log(`TTS Flushed (pending: ${ttsPendingFlushes})`);
                
                // Resolve all waiting promises when no more pending flushes
                if (ttsPendingFlushes === 0 && ttsFlushResolvers.length > 0) {
                  console.log('All flushes complete - resolving waiters');
                  ttsFlushResolvers.forEach(r => r());
                  ttsFlushResolvers = [];
                }
              }
            } catch (e) {}
          }
        };
        
        ttsSocket.onerror = (error) => {
          console.error('TTS WebSocket error:', error);
          connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
          
          // Explicitly close the connection before nulling (ensures Deepgram registers disconnect)
          if (ttsSocket) {
            try {
              ttsSocket.close(1000, 'Error cleanup');
            } catch (e) {}
          }
          ttsSocket = null;
          
          if (!settled) {
            settled = true;
            reject(new Error('TTS connection failed'));
          }
        };
        
        ttsSocket.onclose = () => {
          console.log(`TTS WebSocket closed (total TTS connections created: ${connectionStats.ttsConnectionsCreated})`);
          connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
          ttsSocket = null;
          // Resolve any waiting promises on close
          ttsFlushResolvers.forEach(r => r());
          ttsFlushResolvers = [];
        };
        
        setTimeout(() => {
          if (!settled && ttsSocket && ttsSocket.readyState !== WebSocket.OPEN) {
            settled = true;
            connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
            ttsSocket.close();
            reject(new Error('TTS connection timeout'));
          }
        }, 10000);
      });
    }
    
    // Connect to Deepgram TTS with retry logic (follows Deepgram's 429 guidance)
    async function connectTTS(token) {
      // 1. If already connected and open, REUSE it (don't create new)
      if (ttsSocket && ttsSocket.readyState === WebSocket.OPEN) {
        console.log(`Reusing existing TTS connection (total TTS connections created: ${connectionStats.ttsConnectionsCreated})`);
        return;
      }
      
      // 2. Close old streams before opening new ones (Deepgram guidance)
      if (ttsSocket && ttsSocket.readyState !== WebSocket.OPEN) {
        console.warn(`Closing stale TTS socket (state: ${ttsSocket.readyState}) before opening new one`);
        try {
          ttsSocket.close(1000, 'Cleanup before new connection');
        } catch (e) {}
        ttsSocket = null;
        connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
        // Wait a moment for Deepgram to register the close
        await new Promise(r => setTimeout(r, 500));
      }
      
      // 3. Wait for slot to be available (never exceed concurrent limit)
      await waitForTTSSlot(10000);
      
      // 4. Retry loop with exponential backoff (Deepgram guidance: 1s, 2s, 4s, 8s)
      const maxRetries = 4;
      for (let attempt = 0; attempt <= maxRetries; attempt++) {
        // Enforce minimum interval between connection attempts
        const now = Date.now();
        const timeSinceLastConnect = now - lastTTSConnectTime;
        if (timeSinceLastConnect < TTS_MIN_CONNECT_INTERVAL) {
          const waitTime = TTS_MIN_CONNECT_INTERVAL - timeSinceLastConnect;
          console.log(`Throttling: waiting ${waitTime}ms before TTS connect`);
          await new Promise(r => setTimeout(r, waitTime));
        }
        
        try {
          await tryConnectTTS(token);
          // Success - clear rate limit flag
          if (voiceRateLimited) {
            voiceRateLimited = false;
            hideRateLimitWarning();
          }
          return;
        } catch (error) {
          if (attempt < maxRetries) {
            // Exponential backoff: 1s, 2s, 4s, 8s (as per Deepgram guidance)
            const backoff = Math.pow(2, attempt) * 1000; // 1s, 2s, 4s, 8s
            console.log(`TTS connection failed, retrying in ${backoff}ms (attempt ${attempt + 1}/${maxRetries})`);
            
            // Show rate limit warning on first failure
            if (!voiceRateLimited) {
              voiceRateLimited = true;
              showRateLimitWarning();
            }
            
            await new Promise(r => setTimeout(r, backoff));
          } else {
            // All retries failed - keep warning visible
            voiceRateLimited = true;
            showRateLimitWarning();
            throw new Error('Voice temporarily unavailable - too many connections. Please wait a moment.');
          }
        }
      }
    }
    
    // Send flush and wait for completion (like backend flush_and_wait)
    async function flushAndWait(timeout = 30000) {
      if (!ttsSocket || ttsSocket.readyState !== WebSocket.OPEN) {
        return;
      }
      
      // Increment pending count BEFORE sending
      ttsPendingFlushes++;
      
      // Send flush command
      ttsSocket.send(JSON.stringify({ type: 'Flush' }));
      console.log(`Sent Flush (pending: ${ttsPendingFlushes})`);
      
      // If already at 0 pending (shouldn't happen, but safety)
      if (ttsPendingFlushes === 0) {
        return;
      }
      
      // Wait for flush completion
      return new Promise((resolve) => {
        ttsFlushResolvers.push(resolve);
        
        // Timeout fallback
        setTimeout(() => {
          const idx = ttsFlushResolvers.indexOf(resolve);
          if (idx !== -1) {
            console.log('TTS flush timeout');
            ttsFlushResolvers.splice(idx, 1);
            resolve();
          }
        }, timeout);
      });
    }
    
    // Close TTS connection
    function closeTTS() {
      // Resolve any pending flush waiters
      ttsFlushResolvers.forEach(r => r());
      ttsFlushResolvers = [];
      ttsPendingFlushes = 0;
      
      if (ttsSocket) {
        console.log('Closing TTS connection');
        try {
          ttsSocket.close();
        } catch (e) {}
        connectionStats.ttsConnections = Math.max(0, connectionStats.ttsConnections - 1);
        ttsSocket = null;
      }
    }
    
    // Process transcript with LLM and speak response
    async function processAndSpeak(transcript) {
      isProcessingResponse = true;
      
      try {
        // Connect TTS
        await connectTTS(deepgramToken);
        voiceState = 'speaking';
        
        // Stream LLM response and send to TTS
        const response = await fetch('/api/voice/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            message: transcript,
            session_id: sessionId
          })
        });
        
        if (!response.ok) {
          throw new Error('Failed to get response');
        }
        
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        
        while (true) {
          const { done, value } = await reader.read();
          
          if (audioInterrupted) {
            console.log('Response interrupted');
            break;
          }
          
          if (done) break;
          
          const text = decoder.decode(value);
          buffer += text;
          
          // Send complete sentences to TTS
          while (true) {
            let endIdx = -1;
            for (const punct of ['. ', '! ', '? ', '.\n', '!\n', '?\n']) {
              const idx = buffer.indexOf(punct);
              if (idx !== -1 && (endIdx === -1 || idx < endIdx)) {
                endIdx = idx + punct.length;
              }
            }
            
            if (endIdx === -1) break;
            
            const sentence = buffer.slice(0, endIdx);
            buffer = buffer.slice(endIdx);
            
            if (sentence.trim() && ttsSocket && ttsSocket.readyState === WebSocket.OPEN) {
              ttsSocket.send(JSON.stringify({ type: 'Speak', text: sentence }));
            }
          }
        }
        
        // Send remaining text
        if (buffer.trim() && ttsSocket && ttsSocket.readyState === WebSocket.OPEN && !audioInterrupted) {
          ttsSocket.send(JSON.stringify({ type: 'Speak', text: buffer }));
        }
        
        // Flush TTS and wait for all audio to be generated
        if (ttsSocket && ttsSocket.readyState === WebSocket.OPEN && !audioInterrupted) {
          await flushAndWait(15000); // 15 second timeout
          console.log('Flush complete');
        }
        
      } catch (error) {
        console.error('Error in processAndSpeak:', error);
        // Only close TTS on error
        closeTTS();
      } finally {
        // Don't close TTS here - keep it open for reuse to avoid rate limits
        // TTS will be closed when call ends or on interruption
        isProcessingResponse = false;
        if (!audioInterrupted) {
          voiceState = 'listening';
        }
      }
    }
    
    // Initialize AudioContext for playback (at playback sample rate)
    let playbackContext = null;
    function initPlaybackContext() {
      if (!playbackContext) {
        playbackContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000
        });
      }
      if (playbackContext.state === 'suspended') {
        playbackContext.resume();
      }
      return playbackContext;
    }
    
    // Convert Linear16 PCM bytes to AudioBuffer
    function pcmToAudioBuffer(pcmData) {
      const ctx = initPlaybackContext();
      
      // Linear16 PCM is 16-bit signed integers
      const int16Array = new Int16Array(pcmData.buffer, pcmData.byteOffset, pcmData.byteLength / 2);
      const float32Array = new Float32Array(int16Array.length);
      
      // Convert to float32 (-1.0 to 1.0)
      for (let i = 0; i < int16Array.length; i++) {
        float32Array[i] = int16Array[i] / 32768.0;
      }
      
      // Create AudioBuffer
      const audioBuffer = ctx.createBuffer(1, float32Array.length, 16000);
      audioBuffer.getChannelData(0).set(float32Array);
      
      return audioBuffer;
    }
    
    // Seamless audio playback using scheduled timing
    let nextPlayTime = 0;
    let activeSources = [];  // Track ALL active audio sources
    
    function queueAudio(audioBuffer) {
      const ctx = initPlaybackContext();
      
      // If this is the first chunk or we're behind, start from now
      const currentTime = ctx.currentTime;
      if (nextPlayTime < currentTime) {
        nextPlayTime = currentTime;
      }
      
      // Create and schedule the source
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(ctx.destination);
      
      // Schedule to play at the precise time
      source.start(nextPlayTime);
      
      // Track this source for stopping during interruption
      activeSources.push(source);
      isPlayingAudio = true;
      
      // Update next play time to be right after this buffer ends
      nextPlayTime += audioBuffer.duration;
      
      // Clean up when this source ends
      source.onended = () => {
        // Remove from active sources
        const idx = activeSources.indexOf(source);
        if (idx > -1) {
          activeSources.splice(idx, 1);
        }
        // Mark as not playing if no more sources
        if (activeSources.length === 0) {
          isPlayingAudio = false;
        }
      };
    }
    
    // Stop audio playback (for interruption)
    function stopAudioPlayback() {
      console.log(`Stopping audio playback (${activeSources.length} active sources)`);
      const ctx = initPlaybackContext();
      
      // Stop ALL active sources
      for (const source of activeSources) {
        try {
          source.stop();
        } catch (e) {
          // Source might already be stopped
        }
      }
      activeSources = [];
      
      // Reset timing
      nextPlayTime = ctx.currentTime;
      isPlayingAudio = false;
    }
    
    // Convert Float32 samples to Int16 (Linear16 PCM)
    function float32ToInt16(float32Array) {
      const int16Array = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        // Clamp to -1.0 to 1.0 range and convert to 16-bit
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return int16Array;
    }
    
    // Downsample audio from source rate to target rate
    function downsample(samples, sourceSampleRate, targetSampleRate) {
      if (sourceSampleRate === targetSampleRate) {
        return samples;
      }
      const ratio = sourceSampleRate / targetSampleRate;
      const newLength = Math.round(samples.length / ratio);
      const result = new Float32Array(newLength);
      for (let i = 0; i < newLength; i++) {
        const srcIndex = Math.round(i * ratio);
        result[i] = samples[srcIndex];
      }
      return result;
    }
    
    // Start microphone recording with raw PCM output
    async function startRecording() {
      // Check if already recording
      if (micStream) {
        console.log('Already recording');
        return;
      }
      
      // Check if we're in a secure context (HTTPS or localhost)
      // Note: Chrome treats "localhost" as secure but NOT "127.0.0.1"
      if (!window.isSecureContext) {
        const currentHost = window.location.hostname;
        if (currentHost === '127.0.0.1') {
          throw new Error('Microphone requires a secure context. Please access this site via http://localhost:3001 instead of http://127.0.0.1:3001');
        }
        throw new Error('Microphone access requires HTTPS. Please access this site via HTTPS or http://localhost:3001');
      }
      
      // Check if getUserMedia is available
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('Microphone API not available. Please use a modern browser (Chrome, Firefox, Edge, Safari).');
      }
      
      // Pre-check permission state if available (for better error messages)
      if (navigator.permissions && navigator.permissions.query) {
        try {
          const result = await navigator.permissions.query({ name: 'microphone' });
          console.log('Microphone permission state:', result.state);
          if (result.state === 'denied') {
            throw new Error('Microphone access was blocked. Please click the lock/site-settings icon in your browser\'s address bar, allow microphone access, then refresh the page.');
          }
        } catch (e) {
          // Some browsers don't support querying microphone permission - that's ok
          console.log('Could not query permission state (this is normal):', e.message);
        }
      }
      
      try {
        console.log('Requesting microphone access...');
        micStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          } 
        });
        console.log('Microphone access granted');
        
        // Create audio context for capture
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaStreamSource(micStream);
        
        // Use ScriptProcessorNode to get raw PCM (deprecated but widely supported)
        // Buffer size of 4096 gives ~85ms at 48kHz which we downsample to 16kHz
        const bufferSize = 4096;
        micProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        
        let audioFrameCount = 0;
        micProcessor.onaudioprocess = (event) => {
          // Send to Deepgram STT directly
          if (sttSocket && sttSocket.readyState === WebSocket.OPEN) {
            const inputData = event.inputBuffer.getChannelData(0);
            
            // Downsample to 16kHz if needed
            const downsampled = downsample(inputData, audioContext.sampleRate, 16000);
            
            // Convert to Linear16 PCM
            const pcmData = float32ToInt16(downsampled);
            
            // Send as binary
            sttSocket.send(pcmData.buffer);
            
            // Log occasionally to show audio is flowing
            audioFrameCount++;
            if (audioFrameCount % 50 === 0) { // Every ~4 seconds
              console.log(`Audio frames sent: ${audioFrameCount}`);
            }
          }
        };
        
        // Connect: source -> processor -> destination (required for processor to work)
        source.connect(micProcessor);
        micProcessor.connect(audioContext.destination);
        
        console.log('Recording started (raw PCM at 16kHz)');
        
      } catch (error) {
        console.error('Error starting recording:', error);
        
        // Provide specific error messages
        if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
          throw new Error('Microphone permission denied. Please allow microphone access in your browser settings and try again.');
        } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
          throw new Error('No microphone found. Please connect a microphone and try again.');
        } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
          throw new Error('Microphone is already in use by another application. Please close other apps using the microphone.');
        } else if (error.name === 'OverconstrainedError') {
          throw new Error('Microphone does not support the required settings. Please try a different microphone.');
        } else {
          throw new Error(`Microphone error: ${error.message || error.name}`);
        }
      }
    }
    
    // Stop microphone recording
    function stopRecording() {
      if (micProcessor) {
        micProcessor.disconnect();
        micProcessor = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      if (micStream) {
        micStream.getTracks().forEach(track => track.stop());
        micStream = null;
      }
      console.log('Recording stopped');
    }
    
    // Disconnect all voice connections
    function disconnectVoice() {
      console.log('Disconnecting all voice connections...');
      
      // Close STT
      if (sttSocket) {
        try {
          sttSocket.send(JSON.stringify({ type: 'CloseStream' }));
          sttSocket.close();
        } catch (e) {}
        connectionStats.sttConnections = Math.max(0, connectionStats.sttConnections - 1);
        sttSocket = null;
      }
      
      // Close TTS
      closeTTS();
      
      deepgramToken = null;
      isProcessingResponse = false;
      
      // Log final stats
      console.log('Connection stats:', {
        sttConnections: connectionStats.sttConnections,
        ttsConnections: connectionStats.ttsConnections,
        sttConnectionsCreated: connectionStats.sttConnectionsCreated,
        ttsConnectionsCreated: connectionStats.ttsConnectionsCreated
      });
    }
    
    // Debug function to check connection status (call from browser console)
    window.checkVoiceConnections = function() {
      const status = {
        stt: {
          exists: sttSocket !== null,
          state: sttSocket ? sttSocket.readyState : null,
          stateName: sttSocket ? ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][sttSocket.readyState] : null
        },
        tts: {
          exists: ttsSocket !== null,
          state: ttsSocket ? ttsSocket.readyState : null,
          stateName: ttsSocket ? ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][ttsSocket.readyState] : null
        },
        stats: { ...connectionStats },
        voiceState: voiceState,
        isProcessingResponse: isProcessingResponse
      };
      console.table(status);
      return status;
    };
    
    // Test function to verify connection management (call from browser console)
    // Usage: await testConnectionManagement()
    window.testConnectionManagement = async function() {
      console.log('=== Testing Connection Management ===');
      
      // Get token
      const token = await getDeepgramToken();
      console.log('Token obtained');
      
      // Test 1: Create and reuse TTS connection
      console.log('\nTest 1: Create TTS connection');
      await connectTTS(token);
      checkVoiceConnections();
      
      console.log('\nTest 2: Reuse existing TTS connection');
      await connectTTS(token);
      checkVoiceConnections();
      
      // Test 3: Close and recreate
      console.log('\nTest 3: Close and recreate TTS connection');
      closeTTS();
      await new Promise(r => setTimeout(r, 3500)); // Wait for rate limit window
      await connectTTS(token);
      checkVoiceConnections();
      
      // Cleanup
      console.log('\nCleaning up...');
      disconnectVoice();
      checkVoiceConnections();
      
      console.log('\n=== Test Complete ===');
      console.log('Expected: Only 2-3 TTS connections created (not 5+)');
    };
    
    // Start voice call
    async function startVoiceCall() {
      try {
        // Clean up any existing connections first
        disconnectVoice();
        
        // Reset all state first (in case previous call didn't end cleanly)
        isProcessingResponse = false;
        audioInterrupted = false;
        voiceState = 'idle';
        ttsPendingFlushes = 0;
        ttsFlushResolvers = [];
        lastTTSConnectTime = 0; // Reset rate limit timer
        
        // Clear any previous rate limit warning (fresh start)
        voiceRateLimited = false;
        hideRateLimitWarning();
        
        console.log('Starting new voice call - connection stats reset');
        
        // Initialize playback context (needs user gesture)
        initPlaybackContext();
        
        // Get Deepgram token first
        console.log('Getting Deepgram token...');
        deepgramToken = await getDeepgramToken();
        console.log('Token received');
        
        // Request microphone permission
        console.log('Requesting microphone permission...');
        await startRecording();
        console.log('Microphone access granted');
        
        // Connect to Deepgram STT
        console.log('Connecting to speech recognition...');
        await connectSTT(deepgramToken);
        
        console.log('Voice call started');
        return true;
      } catch (error) {
        console.error('Failed to start voice call:', error);
        
        // Cleanup on failure
        stopRecording();
        disconnectVoice();
        
        let errorMessage = error.message || 'Unknown error';
        if (!errorMessage.includes('Failed to start')) {
          errorMessage = 'Failed to start voice call: ' + errorMessage;
        }
        
        throw new Error(errorMessage);
      }
    }
    
    // End voice call
    function endVoiceCall() {
      stopRecording();
      stopAudioPlayback();
      disconnectVoice();
      voiceState = 'idle';
      audioInterrupted = false;
      console.log('Voice call ended');
    }

    // Floating call button
    callFab.addEventListener('click', async () => {
      const callIcon = callFab.querySelector('.call-icon');
      const endCallIcon = callFab.querySelector('.end-call-icon');
      const tooltip = callFab.querySelector('.fab-tooltip');
      
      if (!isInCall) {
        // Start call - trigger call answer flow
        isInCall = true;
        
        // Hide other FABs
        downloadFab.classList.add('fab-hidden');
        chatFab.classList.add('fab-hidden');
        
        // Switch to X icon
        callIcon.style.display = 'none';
        endCallIcon.style.display = 'block';
        tooltip.textContent = 'End Call';
        callFab.classList.add('call-active');
        
        // Trigger call answer video
        callEndVideo.classList.remove('active');
        callEndVideo.pause();
        callAnswerVideo.currentTime = 0;
        callAnswerVideo.classList.add('active');
        callAnswerVideo.play();
        
        // Start voice call
        try {
          await startVoiceCall();
        } catch (error) {
          // Revert UI on failure
          isInCall = false;
          downloadFab.classList.remove('fab-hidden');
          chatFab.classList.remove('fab-hidden');
          callIcon.style.display = 'block';
          endCallIcon.style.display = 'none';
          tooltip.textContent = 'Start Call';
          callFab.classList.remove('call-active');
          hideAllOverlays();
          alert(error.message || 'Failed to start voice call. Please check microphone permissions.');
        }
        
      } else {
        // End call - trigger call end flow
        isInCall = false;
        
        // End voice call
        endVoiceCall();
        
        // Show other FABs
        downloadFab.classList.remove('fab-hidden');
        chatFab.classList.remove('fab-hidden');
        
        // Switch back to phone icon
        callIcon.style.display = 'block';
        endCallIcon.style.display = 'none';
        tooltip.textContent = 'Start Call';
        callFab.classList.remove('call-active');
        
        // Trigger call end video
        callEndVideo.currentTime = 0;
        callEndVideo.classList.add('active');
        callEndVideo.play().then(() => {
          setTimeout(() => {
            callAnswerVideo.classList.remove('active');
            callAnswerVideo.pause();
          }, 200);
        });
        
        callEndVideo.addEventListener('ended', function hideCallEnd() {
          callEndVideo.classList.remove('active');
          callEndVideo.removeEventListener('ended', hideCallEnd);
        }, { once: true });
      }
    });
  </script>

</body>
</html>